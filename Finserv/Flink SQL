# Workshop Instructions

## 1. Create a free account
Sign up for Confluent Cloud at [Confluent Cloud - Try Free](https://www.confluent.io/confluent-cloud/tryfree/).

---

## 2. Create a new environment
- After logging in, click on **'Add new environment'**.
- Give it a meaningful name (e.g., `<your-name>-environment`).
- Select the **Essentials** package for Stream Governance.

---

## 3. Set up a new cluster
- Click on **'Add cluster'** and select a **Basic Cluster** for the workshop.
- Choose your preferred **cloud provider** and **region**, then click **Launch**.
- Your Confluent Cloud Kafka cluster is now up and running!

---

## 4. Creating a Datagen Connector to Pull Data in Confluent Cloud

### Create a new topic:
- Navigate to the **Topics** tab and click **Create New Topic**.
- Name the topic `trades-data`.

### Add a sample data connector:
- Go to the **Connectors** tab, click **Add Connector**, and search for the **Sample Data** connector.
- Select `trades-data` as the **target topic**.
- Set **Output Message Format** to **Avro**.

### Configure the connector:
- In the **Advanced Configuration** section, choose **Trades** as the dataset.
- Leave the other settings as default and start the connector provisioning.
- Verify data ingestion into the `trades-data` topic.

---

### Create another topic:
- Navigate to the **Topics** tab and click **Create New Topic**.
- Name the topic `users-data`.

### Add another sample data connector:
- Go to the **Connectors** tab, click **Add Connector**, and search for the **Sample Data** connector.
- Select `users-data` as the target topic and set **Output Message Format** to **Avro**.

### Configure the connector:
- In the **Advanced Configuration** section, choose **Users** as the dataset.
- Start the connector provisioning and verify data ingestion into the `users-data` topic.

---

## 5. Creating a Compute Pool in Flink

### Navigate to the Flink tab:
- Hover over the **Flink** tab and click **Create Compute Pool**.

### Configure the compute pool:
- Ensure the region matches your Kafka cluster's region.
- Choose your preferred **cloud provider** and **region**.
- Leave the **Max CFU** setting as default.
- Provide a meaningful name for your compute pool.
- Your compute pool will be up and running in a few minutes.

---

## 6. Creating Tables in Flink

### View the `trade_data` table:
Run the following command to view the table structure:

```sql
SHOW CREATE TABLE trade_data;
```

```sql
Output : CREATE TABLE `trade_data` (
  `key` VARBINARY(2147483647),
  `side` VARCHAR(2147483647) NOT NULL COMMENT 'A simulated trade side (buy or sell or short)',
  `quantity` INT NOT NULL COMMENT 'A simulated random quantity of the trade',
  `symbol` VARCHAR(2147483647) NOT NULL COMMENT 'Simulated stock symbols',
  `price` INT NOT NULL COMMENT 'A simulated random trade price in pennies',
  `account` VARCHAR(2147483647) NOT NULL COMMENT 'Simulated accounts assigned to the trade',
  `userid` VARCHAR(2147483647) NOT NULL COMMENT 'The simulated user who executed the trade'
) DISTRIBUTED BY HASH(`key`) INTO 3 BUCKETS
WITH (
  'changelog.mode' = 'append',
  'connector' = 'confluent',
  'kafka.cleanup-policy' = 'delete',
  'kafka.max-message-size' = '2097164 bytes',
  'kafka.retention.size' = '0 bytes',
  'kafka.retention.time' = '7 d',
  'key.format' = 'raw',
  'scan.bounded.mode' = 'unbounded',
  'scan.startup.mode' = 'earliest-offset',
  'value.format' = 'avro-registry'
);
```

2. Let's add all the data from trade_data table to trades_topic , please run the below command 

CREATE TABLE `Nidhi-workspace`.`flinkdemo-finserv`.`trades_topic` (
  `key` VARBINARY(2147483647),
  `side` VARCHAR(2147483647) NOT NULL COMMENT 'A simulated trade side (buy or sell or short)',
  `quantity` INT NOT NULL COMMENT 'A simulated random quantity of the trade',
  `symbol` VARCHAR(2147483647) NOT NULL COMMENT 'Simulated stock symbols',
  `price` INT NOT NULL COMMENT 'A simulated random trade price in pennies',
  `account` VARCHAR(2147483647) NOT NULL COMMENT 'Simulated accounts assigned to the trade',
  `userid` VARCHAR(2147483647) NOT NULL COMMENT 'The simulated user who executed the trade'
) ;

insert into trades_topic select * from trade_data;

Filtering in Flink : 

Filtering the trade data which has price and quantity greater than 0 .

Create a table filtered_trades

CREATE TABLE `Nidhi-workspace`.`flinkdemo-finserv`.`filtered_trades` (
  `key` VARBINARY(2147483647),
  `side` VARCHAR(2147483647) NOT NULL COMMENT 'A simulated trade side (buy or sell or short)',
  `quantity` INT NOT NULL COMMENT 'A simulated random quantity of the trade',
  `symbol` VARCHAR(2147483647) NOT NULL COMMENT 'Simulated stock symbols',
  `price` INT NOT NULL COMMENT 'A simulated random trade price in pennies',
  `account` VARCHAR(2147483647) NOT NULL COMMENT 'Simulated accounts assigned to the trade',
  `userid` VARCHAR(2147483647) NOT NULL COMMENT 'The simulated user who executed the trade'
) ;

Insert the data by filtering 

INSERT INTO filtered_trades
SELECT *
FROM trades_topic
WHERE quantity > 0
  AND price > 0
  AND (side = 'BUY' OR side = 'SELL');

Joining two tables users data and trades data in Flink using inner join :

SELECT 
    t.side,
    t.quantity,
    t.symbol,
    t.price,
    t.account,
    t.userid,
    u.registertime,
    u.regionid,
    u.gender
FROM 
    trades_topic t
INNER JOIN 
    users_data u 
ON 
    t.userid = u.userid;


Running Aggregates and Window functions 

We will run a tumbling window function which calaculates the total volume of the shares traded and sum of the price of each user for every 10 minutes 

Creating the table broker_trade_volume 

CREATE TABLE broker_trade_volume (
     window_start TIMESTAMP(3),
    window_end TIMESTAMP(3),
  userid STRING,
    total_number_of_shares BIGINT,
  total_amount_traded bigint
  
);

Running aggregate with windowing 

Insert into broker_trade_volume
SELECT window_start, window_end, userid , SUM(quantity) as `total_number_of_shares`, sum(price) as total_amount_traded
  FROM TABLE(
    TUMBLE(TABLE filtered_trades, DESCRIPTOR($rowtime), INTERVAL '10' minutes))
  GROUP BY userid , window_start, window_end;

The output of the above query is inserted into broker_trade_volume topic . I want to push the data into MongoDB for storing . 

Prerequisite : Have a Mongo Atlas cluster  running 

Creating Mongo Atlas Sink connector : 

Follow these steps to set up the MongoDB Atlas Sink connector:

Navigate to the Connectors tab:

Hover over the Connectors tab and select the MongoDB Atlas Sink Connector.
Select the topic:

Choose the topic you want to push data from. In this case, itâ€™s the broker_trade_volume topic.
Provide authentication details:

Enter the required details such as hostname, database username, password, database name, and collection name.
Configuration and validation:

The system will internally validate the connector's connection to MongoDB Atlas. You can leave the remaining settings as default.
After a few seconds, your MongoDB connector will be running, and you can verify the data ingestion into your MongoDB Atlas cluster.

Below is the connector configuration for you to verify 

Configuration : 
{
  "config": {
    "connector.class": "MongoDbAtlasSink",
    "name": "MongoDbAtlasSinkConnector_1",
    "schema.context.name": "default",
    "input.data.format": "AVRO",
    "cdc.handler": "None",
    "value.subject.name.strategy": "TopicNameStrategy",
    "delete.on.null.values": "false",
    "max.batch.size": "0",
    "bulk.write.ordered": "true",
    "rate.limiting.timeout": "0",
    "rate.limiting.every.n": "0",
    "write.strategy": "DefaultWriteModelStrategy",
    "kafka.auth.mode": "KAFKA_API_KEY",
    "kafka.api.key": "MNQKPKTJSJUAX3DH",
    "kafka.api.secret": "****************************************************************",
    "topics": "broker_trade_volume",
    "connection.host": "cflt-test.5afyk.mongodb.net",
    "connection.user": "test-cflt",
    "connection.password": "*********",
    "database": "trade-data",
    "collection": "trade_data_volume",
    "doc.id.strategy": "BsonOidStrategy",
    "doc.id.strategy.overwrite.existing": "false",
    "document.id.strategy.uuid.format": "string",
    "key.projection.type": "none",
    "value.projection.type": "none",
    "namespace.mapper.class": "DefaultNamespaceMapper",
    "server.api.deprecation.errors": "false",
    "server.api.strict": "false",
    "max.num.retries": "3",
    "retries.defer.timeout": "5000",
    "timeseries.timefield.auto.convert": "false",
    "timeseries.timefield.auto.convert.date.format": "yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]",
    "timeseries.timefield.auto.convert.locale.language.tag": "en",
    "timeseries.expire.after.seconds": "0",
    "ts.granularity": "None",
    "max.poll.interval.ms": "300000",
    "max.poll.records": "500",
    "tasks.max": "1"
  }
}

Please note that the user should have read write access on the database , collection specified in the connector . The data flows from Confluent Cloud to Mongo Atlas trade_data_volume collection . 
Verify the data inside the Mongo Atlas UI . 
